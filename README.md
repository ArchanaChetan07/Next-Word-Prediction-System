# Next-Word-Prediction-System
Next Word Prediction with LSTM

This project demonstrates a Next Word Prediction model using an LSTM (Long Short-Term Memory) neural network trained on Hamlet by William Shakespeare. The model predicts the most probable next word in a sequence, and is deployed via a Streamlit web application.

ğŸš€ Features

Deep Learning Model: Built with TensorFlow/Keras LSTM.

Dataset: Text corpus from Hamlet (hamlet.txt)
.

Tokenizer: Pre-trained tokenizer stored in tokenizer.pickle.

Trained Model: Pre-trained weights in next_word_lstm.h5.

Interactive UI: Streamlit app (app.py) lets users enter text and predict the next word in real time
.

Notebook Experiments: Exploratory training and evaluation are in experiments.ipynb.

ğŸ“‚ Project Structure
â”œâ”€â”€ app.py                 # Streamlit app for next word prediction
â”œâ”€â”€ experiments.ipynb      # Jupyter Notebook for training & analysis
â”œâ”€â”€ hamlet.txt             # Training corpus (Hamlet by Shakespeare)
â”œâ”€â”€ next_word_lstm.h5      # Trained LSTM model
â”œâ”€â”€ tokenizer.pickle       # Tokenizer used for preprocessing
â”œâ”€â”€ requirements.txt       # List of dependencies
â””â”€â”€ README.md              # Project documentation

ğŸ› ï¸ Installation

Install dependencies

pip install -r requirements.txt

Dependencies include TensorFlow, NumPy, Pandas, Scikit-learn, Matplotlib, Streamlit, and SciKeras

 Usage

Run the Streamlit app

streamlit run app.py


Interact with the app

Enter a starting sequence of words.

Click Predict Next Word.

The model outputs the predicted next word.

Model Details

Architecture: LSTM-based language model.

Training Data: Tokenized sequences from hamlet.txt.

Sequence Length: Derived from model input shape at runtime

Output: Predicts the most likely next word using softmax probabilities.

Experiments

The experiments.ipynb notebook contains:

Data preprocessing (tokenization, sequence padding).

Model training with early stopping.

Accuracy and loss visualization.

Hyperparameter tuning.

Future Improvements

Extend dataset with larger corpora for better generalization.

Implement beam search for multi-word predictions.

Add API endpoints for external integration.

Support for top-k or temperature-based sampling.
